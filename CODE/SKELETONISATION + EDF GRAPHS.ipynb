{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2cdd508-3919-46c7-9f10-574edb2e3526",
   "metadata": {},
   "source": [
    "## **Skeletonisation and Exponential Decay Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d3e5927-ead0-45d6-b61b-4472daa29ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-TIFF pipeline on the CENTRAL SQUARE crop (width = height of image)\n",
    "# Steps: crop -> polarity auto-select -> remove specks -> centroids -> adaptive/connectivity dilation -> skeletonize\n",
    "# Preview PNG: 2-px thicker (visual only). Exponential plot: blue dots, red line, green dashed.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import convolve, distance_transform_edt\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import (\n",
    "    binary_dilation, binary_closing, remove_small_objects,\n",
    "    disk, skeletonize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96eb7f4a-01b8-4e59-9753-73485127ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch lengths from skeleton (needs skan library)\n",
    "\"\"\"\n",
    "trying to import skan for measuring branch lengths\n",
    "if it doesnt work just install it with pip install skan\n",
    "\"\"\"\n",
    "try:\n",
    "    from skan import Skeleton, summarize\n",
    "    SKAN_AVAILABLE = True\n",
    "except:\n",
    "    SKAN_AVAILABLE = False\n",
    "    print(\"skan not installed - need it for proper branch lengths\")\n",
    "    print(\"run: pip install skan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce84c619-9e45-488c-b787-34c27c664d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config section change these paths for your computer\n",
    "\"\"\"\n",
    "setting up all the file paths and folders\n",
    "input_tif is the image i want to analyze\n",
    "the output folders are where results get saved\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "notebook_dir = Path.cwd()\n",
    "proj_root = notebook_dir.parent\n",
    "org_dir = str(proj_root)\n",
    "BASE_DIR = str(proj_root / \"CRYO-SEM DATA\" / \"CRYO-SEM X30000\")\n",
    "\n",
    "input_tif = os.path.join(BASE_DIR , \"SAMPLE TEST IMAGE X30000.tif\")\n",
    "\n",
    "out_skeleton_img_dir = os.path.join(org_dir , \"SKELETON +EDF/SKELETON\")\n",
    "out_csv_dir          = os.path.join(org_dir , \"SKELETON +EDF/ SKELETON +EDF/ CSVS\")\n",
    "out_decay_dir        = os.path.join(org_dir , \"SKELETON +EDF/EXPONENTIAL DECAY FITTING\")\n",
    "\n",
    "# make sure all the output folders exist\n",
    "os.makedirs(out_skeleton_img_dir, exist_ok=True)\n",
    "os.makedirs(out_csv_dir, exist_ok=True)\n",
    "os.makedirs(out_decay_dir, exist_ok=True)\n",
    "\n",
    "# image scale info from the microscope\n",
    "\"\"\"\n",
    "metadata for converting pixels to micrometers\n",
    "4.00 x 3.00 micrometers total size\n",
    "1280 x 961 pixels but i crop width to 961 to make it square\n",
    "\"\"\"\n",
    "width_um, height_um = 4.00, 3.00\n",
    "width_px_meta, height_px_meta = 1280, 961\n",
    "\n",
    "# settings that might need tweaking\n",
    "\"\"\"\n",
    "these are parameters i can adjust if results look weird\n",
    "DEFAULT_MIN_BRANCH_UM ignore really short branches\n",
    "DEFAULT_HEAD_KEEP_PERCENT how much of branch tips to keep\n",
    "DEFAULT_MIN_RADIUS_PX minimum dilation radius\n",
    "\"\"\"\n",
    "DEFAULT_MIN_BRANCH_UM = 0.0\n",
    "DEFAULT_HEAD_KEEP_PERCENT = 100\n",
    "DEFAULT_MIN_RADIUS_PX = 6\n",
    "MAX_SAFE_BINARY_RADIUS_PX = 64\n",
    "PREVIEW_THICKEN_DISK = 2\n",
    "MIN_CC_AREA = 9  # drop tiny specks before centroiding\n",
    "\n",
    "def exponential_decay_func(x, a, b, c):\n",
    "    \"\"\"exponential decay function for curve fitting\"\"\"\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "def dilate_image_safely(pore_map_bool, radius_px, max_safe=MAX_SAFE_BINARY_RADIUS_PX):\n",
    "    \"\"\"\n",
    "    dilate the binary image safely\n",
    "    if radius is too big it uses distance transform instead to avoid memory errors\n",
    "    \"\"\"\n",
    "    if radius_px <= max_safe:\n",
    "        try:\n",
    "            return binary_dilation(pore_map_bool, disk(int(radius_px)))\n",
    "        except MemoryError:\n",
    "            pass  # fallback to distance method\n",
    "    \n",
    "    # use distance transform method for big radius\n",
    "    dist = distance_transform_edt(~pore_map_bool)\n",
    "    return dist <= float(radius_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a2ccc37-b685-4530-8594-ed94459d114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tiff file and crop to central square\n",
    "\"\"\"\n",
    "checking if the input file exists first\n",
    "then loading it and making it grayscale if needed\n",
    "finally cropping to a square shape in the center\n",
    "\"\"\"\n",
    "\n",
    "# check if file actually exists\n",
    "if not os.path.isfile(input_tif):\n",
    "    print(\"Error: cant find the input file at \" + input_tif)\n",
    "    raise FileNotFoundError(\"Input file not found\")\n",
    "\n",
    "# load the image\n",
    "img = imread(input_tif)\n",
    "if img.ndim > 2:\n",
    "    img = img[..., 0]  # take first channel if its color\n",
    "\n",
    "# crop to center square\n",
    "\"\"\"\n",
    "making the image square by cropping the width\n",
    "keeping the full height and centering the width crop\n",
    "this makes analysis consistent across different images\n",
    "\"\"\"\n",
    "H, W = img.shape\n",
    "side = H  # width equals height now\n",
    "x0 = max(0, (W - side) // 2)  # start position for crop\n",
    "x1 = x0 + side  # end position\n",
    "img = img[:, x0:x1]  # do the actual cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c573fa40-5cc6-4197-94d9-53d0000b228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out if structures are bright or dark pixels\n",
    "\"\"\"\n",
    "trying to automatically detect whether the structures show up as \n",
    "bright pixels or dark pixels in the image\n",
    "this saves me from having to check manually every time\n",
    "\"\"\"\n",
    "\n",
    "# get threshold value for splitting bright vs dark\n",
    "try:\n",
    "    t = threshold_otsu(img)  # otsu method usually works well\n",
    "except Exception:\n",
    "    t = float(np.mean(img))  # fallback to average if otsu fails\n",
    "\n",
    "# make binary masks for both possibilities\n",
    "bright_fg = img > t  # foreground is bright pixels\n",
    "dark_fg   = img < t  # foreground is dark pixels\n",
    "\n",
    "def score_mask(mask):\n",
    "    \"\"\"\n",
    "    count how many objects look like real structure pieces\n",
    "    not too tiny and not too huge\n",
    "    \"\"\"\n",
    "    lbl = label(mask, connectivity=2)  # find connected components\n",
    "    if lbl.max() == 0:\n",
    "        return 0  # no objects found\n",
    "    \n",
    "    # get sizes of all objects\n",
    "    areas = np.bincount(lbl.ravel())[1:]  # skip background label 0\n",
    "    total = mask.size\n",
    "    \n",
    "    # set reasonable size limits\n",
    "    area_min = max(MIN_CC_AREA, 3)  # not too small\n",
    "    area_max = max(int(0.01 * total), area_min + 1)  # not too big\n",
    "    \n",
    "    # count objects in good size range\n",
    "    return int(((areas >= area_min) & (areas <= area_max)).sum())\n",
    "\n",
    "# pick whichever polarity gives more reasonable objects\n",
    "chosen_mask = dark_fg if score_mask(dark_fg) >= score_mask(bright_fg) else bright_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc634bad-b1b4-435a-9da9-9254bcc78b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the mask by removing tiny specks\n",
    "\"\"\"\n",
    "getting rid of really small dots that are probably just noise\n",
    "doing this before finding centroids so we dont get random scattered points\n",
    "\"\"\"\n",
    "clean_mask = remove_small_objects(chosen_mask, min_size=MIN_CC_AREA, connectivity=2)\n",
    "\n",
    "# find all the separate objects and get their centers\n",
    "lbl = label(clean_mask, connectivity=2)  # label each connected piece\n",
    "props = regionprops(lbl)  # get properties of each piece\n",
    "\n",
    "# check if we actually found anything useful\n",
    "if len(props) == 0:\n",
    "    print(\"Error: no objects found after cleaning\")\n",
    "    print(\"maybe try changing MIN_CC_AREA or check if polarity is wrong\")\n",
    "    raise RuntimeError(\"No usable components found\")\n",
    "\n",
    "# get image dimensions\n",
    "height_px, width_px = clean_mask.shape  # both should be the same since we made it square\n",
    "\n",
    "# extract the center points of each object\n",
    "\"\"\"\n",
    "centroid gives (row, col) but i want (x, y) coordinates\n",
    "so i flip them with [::-1]\n",
    "then make sure they stay inside the image boundaries\n",
    "\"\"\"\n",
    "centroids = np.array([p.centroid[::-1] for p in props], dtype=float)  # flip to get (x, y)\n",
    "\n",
    "# convert to integer pixel coordinates and keep them in bounds\n",
    "xs = np.clip(np.round(centroids[:, 0]).astype(int), 0, width_px - 1)\n",
    "ys = np.clip(np.round(centroids[:, 1]).astype(int), 0, height_px - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "603b8301-74f9-475d-a55e-16d3c1729796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many micrometers each pixel represents\n",
    "\"\"\"\n",
    "need to convert from pixels to real world measurements\n",
    "using the original image dimensions and field of view size\n",
    "cropping doesnt change the scale just removes some pixels\n",
    "\"\"\"\n",
    "um_per_px_x = width_um  / width_px_meta  # micrometers per pixel in x direction\n",
    "um_per_px_y = height_um / height_px_meta  # micrometers per pixel in y direction\n",
    "um_per_px = float((um_per_px_x + um_per_px_y) / 2.0)  # average the two directions\n",
    "\n",
    "# make a boolean map showing where the pore centers are\n",
    "\"\"\"\n",
    "creating a binary image where True means theres a pore center at that pixel\n",
    "starting with all False then setting True at each centroid location\n",
    "\"\"\"\n",
    "pore_map = np.zeros((height_px, width_px), dtype=bool)  # start with all False\n",
    "pore_map[ys, xs] = True  # mark the pore center locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19fb5bf4-1b56-4feb-aae9-d9b4756b1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how big to make the dilation radius\n",
    "\"\"\"\n",
    "want to connect nearby pores but not merge everything into one blob\n",
    "using the distance between neighboring pores to guess a good starting radius\n",
    "\"\"\"\n",
    "pts = np.column_stack([xs, ys])  # combine x and y coordinates into pairs\n",
    "\n",
    "if len(pts) >= 2:\n",
    "    # find distance to nearest neighbor for each point\n",
    "    dists, _ = cKDTree(pts).query(pts, k=2)  # k=2 gives point itself and nearest neighbor\n",
    "    nn_med = float(np.median(dists[:, 1]))  # take the neighbor distances (skip self at index 0)\n",
    "    radius_start = max(DEFAULT_MIN_RADIUS_PX, int(np.ceil(nn_med / 2.0)))  # start with half the median distance\n",
    "else:\n",
    "    radius_start = DEFAULT_MIN_RADIUS_PX  # fallback if only one point\n",
    "\n",
    "def connect_nearby_pores(pore_map, start_r, max_r=MAX_SAFE_BINARY_RADIUS_PX):\n",
    "    \"\"\"\n",
    "    grow the radius until we get a reasonable number of connected components\n",
    "    dont want too many tiny pieces or one giant blob\n",
    "    \"\"\"\n",
    "    # set a target for max number of separate pieces\n",
    "    target_max_components = max(50, int(0.02 * pore_map.sum()))  # generous cap\n",
    "    best = None\n",
    "    r = int(start_r)\n",
    "    \n",
    "    # try bigger and bigger radius until we connect enough stuff\n",
    "    while r <= max_r:\n",
    "        blobs = dilate_image_safely(pore_map, r)  # grow each pore by radius r\n",
    "        blobs = binary_closing(blobs, footprint=disk(1))  # fill small gaps\n",
    "        n_comp = label(blobs, connectivity=2).max()  # count separate pieces\n",
    "        best = (blobs, r, n_comp)  # save this result\n",
    "        \n",
    "        if n_comp <= target_max_components:\n",
    "            break  # good enough, stop growing\n",
    "        r += 1  # try bigger radius\n",
    "    \n",
    "    return best  # give back the final blobs, radius used, and component count\n",
    "\n",
    "# do the actual connection\n",
    "pore_blobs, radius, n_comp = connect_nearby_pores(pore_map, radius_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b333ab62-d782-412c-a087-9d4e098c0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeletonize analysis mask (1 px)\n",
    "skeleton = skeletonize(pore_blobs).astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b362950-3635-4912-99d0-a4c99ccac127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count junction points in the skeleton\n",
    "\"\"\"\n",
    "a junction is where 3 or more branches meet\n",
    "using a convolution to count neighbors around each pixel\n",
    "\"\"\"\n",
    "\n",
    "# make a kernel that counts neighbors plus gives center pixel extra weight\n",
    "branch_kernel = np.array([[1, 1, 1],\n",
    "                          [1,10, 1],\n",
    "                          [1, 1, 1]])\n",
    "\n",
    "# apply the kernel to count neighbors at each pixel\n",
    "convolved = convolve(skeleton.astype(int), branch_kernel, mode='constant')\n",
    "\n",
    "# find junction points\n",
    "\"\"\"\n",
    "center pixel gets value 10 if its part of skeleton\n",
    "each neighbor adds 1 if its also skeleton\n",
    "so total is 10 + number_of_neighbors\n",
    "if >= 13 that means 10 + 3+ neighbors = junction point\n",
    "\"\"\"\n",
    "junctions = int(np.sum((convolved >= 13) & (skeleton == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49492896-e2e1-4720-9e6c-dca46a0218ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the length of each branch in the skeleton\n",
    "\"\"\"\n",
    "using skan library to trace all the branches and get their lengths\n",
    "converting from pixels to micrometers for real measurements\n",
    "\"\"\"\n",
    "path_lengths_um = []  # start with empty list\n",
    "\n",
    "# check if skan library is installed and working\n",
    "if SKAN_AVAILABLE:\n",
    "    # analyze the skeleton to find all branches\n",
    "    sk = Skeleton(skeleton)\n",
    "    branch_df = summarize(sk, separator=\"_\")  # get dataframe with branch info\n",
    "    \n",
    "    # figure out which column has the distance data\n",
    "    \"\"\"\n",
    "    different versions of skan use different column names\n",
    "    so checking both possibilities\n",
    "    \"\"\"\n",
    "    col = \"branch_distance\" if \"branch_distance\" in branch_df.columns else \"branch-distance\"\n",
    "    \n",
    "    # get branch lengths in pixels\n",
    "    lengths_px = branch_df[col].to_numpy(dtype=float)\n",
    "    \n",
    "    # convert to micrometers\n",
    "    path_lengths_um = (lengths_px * um_per_px)\n",
    "    \n",
    "    # filter out bad values and tiny branches\n",
    "    \"\"\"\n",
    "    removing infinite values and branches that are too short to be real\n",
    "    keeping only reasonable sized branches\n",
    "    \"\"\"\n",
    "    path_lengths_um = path_lengths_um[np.isfinite(path_lengths_um) & (path_lengths_um > DEFAULT_MIN_BRANCH_UM)]\n",
    "    path_lengths_um = path_lengths_um.tolist()  # convert back to list\n",
    "else:\n",
    "    print(\"Skipping branch length export: skan library not available\")\n",
    "\n",
    "# calculate summary statistics\n",
    "total_paths = len(path_lengths_um)  # how many branches we found\n",
    "mean_um = float(np.mean(path_lengths_um)) if path_lengths_um else 0.0  # average length\n",
    "max_um  = float(np.max(path_lengths_um))  if path_lengths_um else 0.0   # longest branch\n",
    "min_um  = float(np.min(path_lengths_um))  if path_lengths_um else 0.0   # shortest branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "002e9a91-cc72-46e2-8288-c3be47dd5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the filename without extension for saving outputs\n",
    "\"\"\"\n",
    "taking just the base filename and removing the .tif part\n",
    "using this as prefix for all the output files\n",
    "\"\"\"\n",
    "prefix = os.path.splitext(os.path.basename(input_tif))[0]\n",
    "\n",
    "# make skeleton thicker for better visibility in preview image\n",
    "\"\"\"\n",
    "the skeleton is only 1 pixel wide which is hard to see\n",
    "making it a bit thicker just for the preview image\n",
    "not changing the actual analysis just making it easier to look at\n",
    "\"\"\"\n",
    "skel_preview = binary_dilation(skeleton, footprint=disk(PREVIEW_THICKEN_DISK))\n",
    "\n",
    "# create and save the skeleton preview image\n",
    "plt.imshow(skel_preview, cmap='gray')  # show in grayscale\n",
    "title_text = \"Skeleton Network\\nJunctions: \" + str(junctions) + \" | Paths: \" + str(total_paths) + \" | Mean Length: \" + str(round(mean_um, 2)) + \" µm\"\n",
    "plt.title(title_text)\n",
    "plt.axis('off')  # hide the axis numbers\n",
    "plt.tight_layout()  # make it look neat\n",
    "\n",
    "# save the image\n",
    "skeleton_img_path = os.path.join(out_skeleton_img_dir, prefix + \"_skeleton_network_detailed.png\")\n",
    "plt.savefig(skeleton_img_path, dpi=300)  # high quality\n",
    "plt.close()  # close to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e914e29b-80f8-47b1-8582-7b5b114e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the path lengths to a csv file\n",
    "\"\"\"\n",
    "if we found any branches then save all their lengths\n",
    "each row will be one branch with its length in micrometers\n",
    "\"\"\"\n",
    "if path_lengths_um:\n",
    "    path_df = pd.DataFrame({\"Path_Length_um\": path_lengths_um})\n",
    "    path_csv_name = prefix + \"_skeleton_path_lengths.csv\"\n",
    "    path_df.to_csv(os.path.join(out_csv_dir, path_csv_name), index=False)\n",
    "\n",
    "# create summary table with all the important results\n",
    "\"\"\"\n",
    "putting all the key numbers in one table\n",
    "includes counts, measurements, and settings used\n",
    "\"\"\"\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Total Junctions\",\n",
    "        \"Total Path Segments\", \n",
    "        \"Mean Path Length (µm)\",\n",
    "        \"Max Path Length (µm)\",\n",
    "        \"Min Path Length (µm)\",\n",
    "        \"µm per pixel (used)\",\n",
    "        \"Dilation radius (px)\",\n",
    "        \"Head kept for fit (%)\",\n",
    "        \"Min branch used (µm)\",\n",
    "        \"Connected components after dilation\",\n",
    "        \"Crop used\",\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        junctions,\n",
    "        total_paths,\n",
    "        round(mean_um, 3),\n",
    "        round(max_um, 3), \n",
    "        round(min_um, 3),\n",
    "        round(float(um_per_px), 6),\n",
    "        int(radius),\n",
    "        int(DEFAULT_HEAD_KEEP_PERCENT),\n",
    "        float(DEFAULT_MIN_BRANCH_UM),\n",
    "        int(n_comp),\n",
    "        \"central square: H=\" + str(height_px) + \", W=\" + str(width_px),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# save the summary table\n",
    "summary_csv_name = prefix + \"_skeleton_summary_metrics.csv\"\n",
    "summary_df.to_csv(os.path.join(out_csv_dir, summary_csv_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59543e7a-8bce-4a78-a33d-fd079019e726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: c:\\Users\\walsh\\Documents\\GitHub\\AGAROSE-HYDROGEL-TRENDS-USING-AI-ML\\SKELETON +EDF/EXPONENTIAL DECAY FITTING\\SAMPLE TEST IMAGE X30000_decay_fit.png\n"
     ]
    }
   ],
   "source": [
    "# try to fit a curve to see how branch lengths change\n",
    "\"\"\"\n",
    "taking all the branch lengths and sorting them\n",
    "then seeing if they follow an exponential pattern\n",
    "like how tree branches get shorter as you go out\n",
    "\"\"\"\n",
    "def exp_decay(x, a, b, c):\n",
    "    \"\"\"Exponential decay function: y = a * exp(-b * x) + c\"\"\"\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "lengths = np.array(path_lengths_um, dtype=float)\n",
    "lengths = lengths[np.isfinite(lengths) & (lengths > 0)]  # remove bad values\n",
    "\n",
    "# need enough data points to make a curve\n",
    "if lengths.size >= 4:\n",
    "    # sort branches from biggest to smallest\n",
    "    lengths_sorted = np.sort(lengths)[::-1]  # flip to get biggest first\n",
    "    ranks = np.arange(len(lengths_sorted))  # just 0, 1, 2, 3...\n",
    "    \n",
    "    # maybe use only the longer branches\n",
    "    if DEFAULT_HEAD_KEEP_PERCENT < 100:\n",
    "        cutoff = np.percentile(lengths_sorted, 100 - DEFAULT_HEAD_KEEP_PERCENT)\n",
    "        mask = lengths_sorted > cutoff\n",
    "        x_data = ranks[mask]\n",
    "        y_data = lengths_sorted[mask]\n",
    "    else:\n",
    "        x_data = ranks\n",
    "        y_data = lengths_sorted\n",
    "    \n",
    "    # guess some starting values for the curve fitting\n",
    "    a_guess = max(y_data) - min(y_data) if len(y_data) else lengths_sorted[0]\n",
    "    c_guess = min(y_data) if len(y_data) else lengths_sorted[-1]\n",
    "    half_way = max(1, int(0.10 * len(x_data)))\n",
    "    b_guess = np.log(2) / half_way\n",
    "    starting_guess = (a_guess, b_guess, c_guess)\n",
    "    \n",
    "    # try to fit the curve\n",
    "    try:\n",
    "        fitted_params, _ = curve_fit(\n",
    "            exp_decay, x_data, y_data,\n",
    "            p0=starting_guess,\n",
    "            bounds=([0, 0, 0], [np.inf, np.inf, np.inf]),\n",
    "            maxfev=20000\n",
    "        )\n",
    "        \n",
    "        # make a reference line for comparison\n",
    "        b_ref = np.log(2) / half_way\n",
    "        a_ref = lengths_sorted[0]\n",
    "        c_ref = lengths_sorted[-1]\n",
    "        reference_line = exp_decay(ranks, a_ref, b_ref, c_ref)\n",
    "        \n",
    "        # draw the plot\n",
    "        fig, ax = plt.subplots(figsize=(9, 6))\n",
    "        ax.scatter(ranks, lengths_sorted, label='Data Points', color='blue')\n",
    "        ax.plot(ranks, exp_decay(ranks, *fitted_params), 'r-', label='Fitted Curve')\n",
    "        ax.plot(ranks, reference_line, 'g--', label='Reference')\n",
    "        \n",
    "        # add labels\n",
    "        title_text = prefix + \" - How Branch Lengths Change\"\n",
    "        ax.set_title(title_text, pad=16)\n",
    "        ax.set_xlabel(\"Branch Number (sorted)\", labelpad=14)\n",
    "        ax.set_ylabel(\"Branch Length (micrometers)\", labelpad=14)\n",
    "        ax.grid(True, alpha=0.4)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5))\n",
    "        plt.tight_layout(rect=[0, 0, 0.80, 1])\n",
    "        \n",
    "        # save the picture\n",
    "        plot_filename = prefix + \"_decay_fit.png\"\n",
    "        decay_png = os.path.join(out_decay_dir, plot_filename)\n",
    "        plt.savefig(decay_png, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Saved plot: \" + decay_png)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Could not fit curve for \" + prefix + \": \" + str(e))\n",
    "        \n",
    "else:\n",
    "    print(prefix + \": not enough branches to fit curve (only \" + str(lengths.size) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e18fc505-4b78-41b8-87a1-14b2db9d421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done processing: c:\\Users\\walsh\\Documents\\GitHub\\AGAROSE-HYDROGEL-TRENDS-USING-AI-ML\\CRYO-SEM DATA\\CRYO-SEM X30000\\SAMPLE TEST IMAGE X30000.tif\n",
      "Used radius: 22px, scale: 0.003123 µm/px\n",
      "\n",
      "Results:\n",
      " Junctions found: 682\n",
      " Path segments: 415\n",
      "\n",
      "Path lengths:\n",
      " Average: 0.13 µm\n",
      " Longest: 0.79 µm\n",
      " Shortest: 0.0 µm\n",
      "\n",
      "Output files:\n",
      " Skeleton image: c:\\Users\\walsh\\Documents\\GitHub\\AGAROSE-HYDROGEL-TRENDS-USING-AI-ML\\SKELETON +EDF/SKELETON\\SAMPLE TEST IMAGE X30000_skeleton_network_detailed.png\n",
      " CSV files saved in: c:\\Users\\walsh\\Documents\\GitHub\\AGAROSE-HYDROGEL-TRENDS-USING-AI-ML\\SKELETON +EDF/ SKELETON +EDF/ CSVS\n"
     ]
    }
   ],
   "source": [
    "# print summary of what we found\n",
    "\"\"\"\n",
    "showing all the main results so we can see if things worked right\n",
    "\"\"\"\n",
    "print(\"\")\n",
    "print(\"Done processing: \" + input_tif)\n",
    "print(\"Used radius: \" + str(radius) + \"px, scale: \" + str(round(um_per_px, 6)) + \" µm/px\")\n",
    "print(\"\")\n",
    "print(\"Results:\")\n",
    "print(\" Junctions found: \" + str(junctions))\n",
    "print(\" Path segments: \" + str(total_paths))\n",
    "print(\"\")\n",
    "print(\"Path lengths:\")\n",
    "print(\" Average: \" + str(round(mean_um, 2)) + \" µm\")\n",
    "print(\" Longest: \" + str(round(max_um, 2)) + \" µm\") \n",
    "print(\" Shortest: \" + str(round(min_um, 2)) + \" µm\")\n",
    "print(\"\")\n",
    "print(\"Output files:\")\n",
    "print(\" Skeleton image: \" + skeleton_img_path)\n",
    "print(\" CSV files saved in: \" + out_csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c063a24-d3b4-42b8-9149-3bf0839f9646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pore-acc (NumPy 1.26)",
   "language": "python",
   "name": "pore-acc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
